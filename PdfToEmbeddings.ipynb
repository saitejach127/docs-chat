{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db2e1912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import mwclient  # for downloading example Wikipedia articles\n",
    "import mwparserfromhell  # for splitting Wikipedia articles into sections\n",
    "import openai  # for generating embeddings\n",
    "import pandas as pd  # for DataFrames to store article sections and embeddings\n",
    "import re  # for cutting <ref> links out of Wikipedia articles\n",
    "import tiktoken  # for counting tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da0f8f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4b36559",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"docs/csm.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e678118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 3384, which is longer than the specified 1000\n",
      "Created a chunk of size 1103, which is longer than the specified 1000\n",
      "Created a chunk of size 1008, which is longer than the specified 1000\n",
      "Created a chunk of size 1075, which is longer than the specified 1000\n",
      "Created a chunk of size 1078, which is longer than the specified 1000\n",
      "Created a chunk of size 1904, which is longer than the specified 1000\n",
      "Created a chunk of size 1061, which is longer than the specified 1000\n",
      "Created a chunk of size 1006, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 to 499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1004, which is longer than the specified 1000\n",
      "Created a chunk of size 1217, which is longer than the specified 1000\n",
      "Created a chunk of size 1863, which is longer than the specified 1000\n",
      "Created a chunk of size 1018, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500 to 999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1735, which is longer than the specified 1000\n",
      "Created a chunk of size 1015, which is longer than the specified 1000\n",
      "Created a chunk of size 1216, which is longer than the specified 1000\n",
      "Created a chunk of size 1073, which is longer than the specified 1000\n",
      "Created a chunk of size 1587, which is longer than the specified 1000\n",
      "Created a chunk of size 1438, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1000 to 1499\n",
      "Batch 1500 to 1999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 3384, which is longer than the specified 1000\n",
      "Created a chunk of size 1103, which is longer than the specified 1000\n",
      "Created a chunk of size 1008, which is longer than the specified 1000\n",
      "Created a chunk of size 1075, which is longer than the specified 1000\n",
      "Created a chunk of size 1078, which is longer than the specified 1000\n",
      "Created a chunk of size 1904, which is longer than the specified 1000\n",
      "Created a chunk of size 1061, which is longer than the specified 1000\n",
      "Created a chunk of size 1006, which is longer than the specified 1000\n",
      "Created a chunk of size 1004, which is longer than the specified 1000\n",
      "Created a chunk of size 1217, which is longer than the specified 1000\n",
      "Created a chunk of size 1863, which is longer than the specified 1000\n",
      "Created a chunk of size 1018, which is longer than the specified 1000\n",
      "Created a chunk of size 1735, which is longer than the specified 1000\n",
      "Created a chunk of size 1015, which is longer than the specified 1000\n",
      "Created a chunk of size 1216, which is longer than the specified 1000\n",
      "Created a chunk of size 1073, which is longer than the specified 1000\n",
      "Created a chunk of size 1587, which is longer than the specified 1000\n",
      "Created a chunk of size 1438, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "# calculate embeddings\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI's best embeddings as of Apr 2023\n",
    "BATCH_SIZE = 500  # you can submit up to 2048 embedding inputs per request\n",
    "text_splitter = NLTKTextSplitter(chunk_size=1000)\n",
    "\n",
    "embeddings = []\n",
    "for batch_start in range(0, len(pages), BATCH_SIZE):\n",
    "    batch_end = batch_start + BATCH_SIZE\n",
    "    batch = [x.page_content for x in pages[batch_start:batch_end]]\n",
    "    batches = []\n",
    "    for t in batch:\n",
    "        batches.extend(text_splitter.split_text(t))\n",
    "    batch=batches\n",
    "    print(f\"Batch {batch_start} to {batch_end-1}\")\n",
    "    response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch)\n",
    "    for i, be in enumerate(response[\"data\"]):\n",
    "        assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
    "    batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "    embeddings.extend(batch_embeddings)\n",
    "    \n",
    "batch = [x.page_content for x in pages]\n",
    "batches = []\n",
    "for t in batch:\n",
    "    batches.extend(text_splitter.split_text(t))\n",
    "batch=batches\n",
    "df = pd.DataFrame({\"text\": batch, \"embedding\": embeddings})\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "143f130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save document chunks and embeddings\n",
    "\n",
    "SAVE_PATH = \"csm_nltk_1.csv\"\n",
    "\n",
    "df.to_csv(SAVE_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33a5fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
